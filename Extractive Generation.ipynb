{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.10.0 in d:\\environments\\ptorch\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: rouge_score in d:\\environments\\ptorch\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: tabulate in d:\\environments\\ptorch\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: tqdm in d:\\environments\\ptorch\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\environments\\ptorch\\lib\\site-packages (from datasets==2.10.0) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in d:\\environments\\ptorch\\lib\\site-packages (from datasets==2.10.0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in d:\\environments\\ptorch\\lib\\site-packages (from datasets==2.10.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in d:\\environments\\ptorch\\lib\\site-packages (from datasets==2.10.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\environments\\ptorch\\lib\\site-packages (from datasets==2.10.0) (2.32.3)\n",
      "Requirement already satisfied: xxhash in d:\\environments\\ptorch\\lib\\site-packages (from datasets==2.10.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in d:\\environments\\ptorch\\lib\\site-packages (from datasets==2.10.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in d:\\environments\\ptorch\\lib\\site-packages (from fsspec[http]>=2021.11.1->datasets==2.10.0) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in d:\\environments\\ptorch\\lib\\site-packages (from datasets==2.10.0) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in d:\\environments\\ptorch\\lib\\site-packages (from datasets==2.10.0) (0.30.2)\n",
      "Requirement already satisfied: packaging in d:\\environments\\ptorch\\lib\\site-packages (from datasets==2.10.0) (24.2)\n",
      "Requirement already satisfied: responses<0.19 in d:\\environments\\ptorch\\lib\\site-packages (from datasets==2.10.0) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\environments\\ptorch\\lib\\site-packages (from datasets==2.10.0) (6.0.2)\n",
      "Requirement already satisfied: absl-py in d:\\environments\\ptorch\\lib\\site-packages (from rouge_score) (2.2.2)\n",
      "Requirement already satisfied: nltk in d:\\environments\\ptorch\\lib\\site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in d:\\environments\\ptorch\\lib\\site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: colorama in d:\\environments\\ptorch\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\environments\\ptorch\\lib\\site-packages (from aiohttp->datasets==2.10.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\environments\\ptorch\\lib\\site-packages (from aiohttp->datasets==2.10.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\environments\\ptorch\\lib\\site-packages (from aiohttp->datasets==2.10.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\environments\\ptorch\\lib\\site-packages (from aiohttp->datasets==2.10.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\environments\\ptorch\\lib\\site-packages (from aiohttp->datasets==2.10.0) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\environments\\ptorch\\lib\\site-packages (from aiohttp->datasets==2.10.0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\environments\\ptorch\\lib\\site-packages (from aiohttp->datasets==2.10.0) (1.19.0)\n",
      "Requirement already satisfied: filelock in d:\\environments\\ptorch\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\environments\\ptorch\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\environments\\ptorch\\lib\\site-packages (from requests>=2.19.0->datasets==2.10.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\environments\\ptorch\\lib\\site-packages (from requests>=2.19.0->datasets==2.10.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\environments\\ptorch\\lib\\site-packages (from requests>=2.19.0->datasets==2.10.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\environments\\ptorch\\lib\\site-packages (from requests>=2.19.0->datasets==2.10.0) (2025.1.31)\n",
      "Requirement already satisfied: click in d:\\environments\\ptorch\\lib\\site-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in d:\\environments\\ptorch\\lib\\site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\environments\\ptorch\\lib\\site-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\environments\\ptorch\\lib\\site-packages (from pandas->datasets==2.10.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\environments\\ptorch\\lib\\site-packages (from pandas->datasets==2.10.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\environments\\ptorch\\lib\\site-packages (from pandas->datasets==2.10.0) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets==2.10.0 rouge_score tabulate tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>From the Judgment and Order dated 25 6 74 of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>718 of 1979 From the Judgment and Order dated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>She has made a prayer that the respondents may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Section 4 provides for an application for fixa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>In other words the proceedings were allegedly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>1015</td>\n",
       "      <td>These two appeals by special leave, one pre fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>1016</td>\n",
       "      <td>The assessee was already in existence but the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1017</td>\n",
       "      <td>1 in CA 1742/69 and for the Appellant in CA 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>1018</td>\n",
       "      <td>Appeal by special leave from the judgment and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>1019</td>\n",
       "      <td>There after a notice of demand in respect of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1020 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                            summary\n",
       "0              0  From the Judgment and Order dated 25 6 74 of t...\n",
       "1              1  718 of 1979 From the Judgment and Order dated ...\n",
       "2              2  She has made a prayer that the respondents may...\n",
       "3              3  Section 4 provides for an application for fixa...\n",
       "4              4  In other words the proceedings were allegedly ...\n",
       "...          ...                                                ...\n",
       "1015        1015  These two appeals by special leave, one pre fe...\n",
       "1016        1016  The assessee was already in existence but the ...\n",
       "1017        1017  1 in CA 1742/69 and for the Appellant in CA 17...\n",
       "1018        1018  Appeal by special leave from the judgment and ...\n",
       "1019        1019  There after a notice of demand in respect of t...\n",
       "\n",
       "[1020 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv('extractive_summaries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since Yashaswat/Indian-Legal-Text-ABS couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\jpsre\\.cache\\huggingface\\datasets\\Yashaswat___indian-legal-text-abs\\default\\0.0.0\\a26ff9215839ebeeb88db6380ea67c8af3875a49 (last modified on Fri Apr  4 15:33:19 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ final_data.csv created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Yashaswat/Indian-Legal-Text-ABS\")\n",
    "\n",
    "judgement_data = dataset['train'].select(range(1020))\n",
    "df_judgements = pd.DataFrame(judgement_data)\n",
    "\n",
    "df_summaries = pd.read_csv(\"extractive_summaries.csv\")   \n",
    "\n",
    "assert len(df_judgements) == 1020\n",
    "assert len(df_summaries) == 1020\n",
    "\n",
    "final_df = pd.DataFrame({\n",
    "    \"judgement\": df_judgements[\"judgement\"],\n",
    "    \"summary\": df_summaries[\"summary\"]\n",
    "})\n",
    "\n",
    "final_df.to_csv(\"final_data.csv\", index=False)\n",
    "\n",
    "print(\"final_data.csv created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judgement</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Appeal No. 623 of 1975.\\nFrom the Judgment and...</td>\n",
       "      <td>From the Judgment and Order dated 25 6 74 of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N: Criminal Appeal No. 718 of 1979 From the Ju...</td>\n",
       "      <td>718 of 1979 From the Judgment and Order dated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it Petition (Civil) No. 623 of 1989.\\n(Under A...</td>\n",
       "      <td>She has made a prayer that the respondents may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Appeals Nos.\\n50 of 1968 and 1201 of 1970.\\nFr...</td>\n",
       "      <td>Section 4 provides for an application for fixa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N: Civil Appeal No. 135 of 1991.\\nFrom the Jud...</td>\n",
       "      <td>In other words the proceedings were allegedly ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           judgement  \\\n",
       "0  Appeal No. 623 of 1975.\\nFrom the Judgment and...   \n",
       "1  N: Criminal Appeal No. 718 of 1979 From the Ju...   \n",
       "2  it Petition (Civil) No. 623 of 1989.\\n(Under A...   \n",
       "3  Appeals Nos.\\n50 of 1968 and 1201 of 1970.\\nFr...   \n",
       "4  N: Civil Appeal No. 135 of 1991.\\nFrom the Jud...   \n",
       "\n",
       "                                             summary  \n",
       "0  From the Judgment and Order dated 25 6 74 of t...  \n",
       "1  718 of 1979 From the Judgment and Order dated ...  \n",
       "2  She has made a prayer that the respondents may...  \n",
       "3  Section 4 provides for an application for fixa...  \n",
       "4  In other words the proceedings were allegedly ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('final_data.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jpsre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\jpsre\\AppData\\Local\\Temp\\ipykernel_6944\\238629700.py:57: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n",
      "100%|██████████| 100/100 [00:00<00:00, 194.12it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 90.22it/s]\n",
      "100%|██████████| 100/100 [00:12<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 ROUGE Evaluation Results:\n",
      "\n",
      "╒══════════╤═══════════╤═══════════╤═══════════╕\n",
      "│ Method   │   ROUGE-1 │   ROUGE-2 │   ROUGE-L │\n",
      "╞══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ TF-IDF   │     48.92 │     16.99 │     21.68 │\n",
      "├──────────┼───────────┼───────────┼───────────┤\n",
      "│ LSA      │     53.48 │     23.81 │     26.81 │\n",
      "├──────────┼───────────┼───────────┼───────────┤\n",
      "│ TextRank │     53.68 │     23.82 │     26.61 │\n",
      "╘══════════╧═══════════╧═══════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import load_metric\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "def tfidf_summarization(text, top_k=5):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) <= top_k:\n",
    "        return text\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
    "    ranked_sentences = np.argsort(sentence_scores)[-top_k:][::-1]\n",
    "    return \" \".join([sentences[i] for i in sorted(ranked_sentences)])\n",
    "\n",
    "def lsa_summarization(text, top_k=5):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) <= top_k:\n",
    "        return text\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    svd = TruncatedSVD(n_components=1)\n",
    "    svd_matrix = svd.fit_transform(tfidf_matrix)\n",
    "    sentence_scores = svd_matrix.flatten()\n",
    "    ranked_sentences = np.argsort(sentence_scores)[-top_k:][::-1]\n",
    "    return \" \".join([sentences[i] for i in sorted(ranked_sentences)])\n",
    "\n",
    "def textrank_summarization(text, top_k=5):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) <= top_k:\n",
    "        return text\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    ranked_sentences = sorted(scores, key=scores.get, reverse=True)[:top_k]\n",
    "    return \" \".join([sentences[i] for i in sorted(ranked_sentences)])\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"final_data.csv\")\n",
    "df = df.dropna(subset=[\"judgement\", \"summary\"])\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def evaluate_rouge(predictions, references):\n",
    "    result = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    formatted = {\n",
    "        \"ROUGE-1\": round(result[\"rouge1\"].mid.fmeasure * 100, 2),\n",
    "        \"ROUGE-2\": round(result[\"rouge2\"].mid.fmeasure * 100, 2),\n",
    "        \"ROUGE-L\": round(result[\"rougeL\"].mid.fmeasure * 100, 2)\n",
    "    }\n",
    "    return formatted\n",
    "\n",
    "results = []\n",
    "\n",
    "def evaluate_method(method_fn, name, top_k=5, limit=100):\n",
    "    predictions, references = [], []\n",
    "    for _, row in tqdm(df.head(limit).iterrows(), total=min(limit, len(df))):\n",
    "        pred = method_fn(row[\"judgement\"], top_k=top_k)\n",
    "        predictions.append(pred)\n",
    "        references.append(row[\"summary\"])\n",
    "    scores = evaluate_rouge(predictions, references)\n",
    "    results.append([name, scores[\"ROUGE-1\"], scores[\"ROUGE-2\"], scores[\"ROUGE-L\"]])\n",
    "\n",
    "evaluate_method(tfidf_summarization, \"TF-IDF\")\n",
    "evaluate_method(lsa_summarization, \"LSA\")\n",
    "evaluate_method(textrank_summarization, \"TextRank\")\n",
    "\n",
    "print(\"\\n ROUGE Evaluation Results:\\n\")\n",
    "print(tabulate(results, headers=[\"Method\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"], tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jpsre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Training: 100%|██████████| 33477/33477 [20:06<00:00, 27.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3 - Loss: 0.2820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 33477/33477 [20:17<00:00, 27.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3 - Loss: 0.2505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 33477/33477 [20:11<00:00, 27.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/3 - Loss: 0.2398\n",
      "\n",
      "Generated Summary:\n",
      " The respondent filed a suit for the grant of a permanent injunction restraining the appellant from interfering with the possession.. The trial Court in the present case rightly said that it could not be said that there was any dispute as to tenancy.. In view of the fact that no costs were al lowed by the High Court, there will be no order as to costs.. This appeal by special leave is from the judgment .dated 25 June, 1974 of the Karnataka High Court.. The question whether the respondent is a tenant or deemed to be a tenant does not at all arise because the tenancy came to an end.. The respondent by notice was called upon to hand over possession of the land immediately after the expiry of the period of lease.. The appellant opposed the application for stay of the suit by the civil court and referring to the Tribunal for decision under the Karnataka Land Reforms Act, 1961.. The High Court reversed the decision of the trial Court and directed the trial Court to refer such of the issues which are required to be.. The reason why the appellant required that land is that the Corporation proposed a scheme for the development and construction of a new township on that area.. The appellant then instituted the suit in appeal claim ing possession from the respondent.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "def preprocess_data(texts, summaries):\n",
    "    processed_texts = []\n",
    "    processed_labels = []\n",
    "\n",
    "    for text, summary in zip(texts, summaries):\n",
    "        text_sentences = nltk.sent_tokenize(str(text))\n",
    "        summary_sentences = nltk.sent_tokenize(str(summary))\n",
    "\n",
    "        labels = [1 if sentence in summary_sentences else 0 for sentence in text_sentences]\n",
    "\n",
    "        processed_texts.extend(text_sentences)\n",
    "        processed_labels.extend(labels)\n",
    "\n",
    "    return processed_texts, processed_labels\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "\n",
    "class ExtractiveSummarizer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, n_heads=8):\n",
    "        super(ExtractiveSummarizer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim * 2, num_heads=n_heads)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=hidden_dim * 2, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.dropout(self.embedding(input_ids)) \n",
    "        lstm_output, _ = self.lstm(embedded)\n",
    "        lstm_output = self.layer_norm(lstm_output)\n",
    "\n",
    "        lstm_output = lstm_output.permute(1, 0, 2)\n",
    "        attn_output, _ = self.attention(lstm_output, lstm_output, lstm_output)\n",
    "        attn_output = attn_output.permute(1, 0, 2)\n",
    "\n",
    "        conv_input = attn_output.permute(0, 2, 1)\n",
    "        conv_output = self.conv1d(conv_input)\n",
    "        conv_output = conv_output.permute(0, 2, 1)\n",
    "\n",
    "        pooled_output = torch.mean(conv_output, dim=1)\n",
    "\n",
    "        output = self.fc(pooled_output)\n",
    "\n",
    "        return torch.sigmoid(output).squeeze(1)\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def generate_summary(model, text, tokenizer, max_len, device, top_k=10):\n",
    "    model.eval()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    summaries = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask)\n",
    "            summaries.append((sentence, output.item()))\n",
    "\n",
    "    summaries.sort(key=lambda x: x[1], reverse=True)\n",
    "    return \". \".join([sentence for sentence, score in summaries[:top_k]])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    VOCAB_SIZE = 30522\n",
    "    EMBEDDING_DIM = 256\n",
    "    HIDDEN_DIM = 128\n",
    "    OUTPUT_DIM = 1\n",
    "    N_LAYERS = 2\n",
    "    DROPOUT = 0.3\n",
    "    MAX_LEN = 1024\n",
    "    BATCH_SIZE = 4\n",
    "    LR = 1e-4\n",
    "    EPOCHS = 3\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "    df = pd.read_csv(\"final_data.csv\")  \n",
    "    df = df.dropna(subset=[\"judgement\", \"summary\"])\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_texts, train_summaries = train_df[\"judgement\"].tolist(), train_df[\"summary\"].tolist()\n",
    "    val_texts, val_summaries = val_df[\"judgement\"].tolist(), val_df[\"summary\"].tolist()\n",
    "\n",
    "\n",
    "    train_texts, train_labels = preprocess_data(train_texts, train_summaries)\n",
    "    val_texts, val_labels = preprocess_data(val_texts, val_summaries)\n",
    "\n",
    "    train_dataset = TextDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "    val_dataset = TextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = ExtractiveSummarizer(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_model(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} - Loss: {train_loss:.4f}\")\n",
    "\n",
    "    example_text = df.iloc[0][\"judgement\"]\n",
    "    summary = generate_summary(model, example_text, tokenizer, MAX_LEN, DEVICE)\n",
    "    print(\"\\nGenerated Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to extractive_summarizer_model.pth\n",
      "Tokenizer saved to 'saved_tokenizer/' directory\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"extractive_summarizer_model.pth\"\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"\\nModel saved to {MODEL_PATH}\")\n",
    "tokenizer.save_pretrained(\"saved_tokenizer\")\n",
    "print(\"Tokenizer saved to 'saved_tokenizer/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Appeal No. 623 of 1975.\n",
    "From the Judgment and Order dated 25 6 74 of the Karnataka 'High Court in Civil Revision No 1981/73.\n",
    "S.S. JavaIi and B.P. Singh, for the Appellants.\n",
    "S.V. Gupte and K.N. Bhatt, for the Respondent.\n",
    "The Judgment of the Court was delivered by RAY, C.J.\n",
    "This appeal by special leave is from the judgment .dated 25 June, 1974 of the Karnataka High Court.\n",
    "The principal question in this appeal whether section 107 of the Karnataka Land Reforms Act, 1961 applies to the land in suit which was leased to the respondent.\n",
    "A large plot of land comprising an area of about 20 acres popularly known as \"The Chamaraja Sewage Farm\" situate in the city of Bangalore belongs to the appellant Corpora tion.\n",
    "The appellant :leased to the respondent by a regis tered lease dated 14 September, 270 1953 the aforementioned land for a period of 5 years on an annual rent of Rs. 13,555/ .\n",
    "The respondent by notice was called upon to hand over possession of the land immediately after the expiry of the period of lease.\n",
    "The respondent failed to deliver possession.\n",
    "The reason why the appellant required that land is that the Corporation proposed a scheme for the development and construction of a new township on that area.\n",
    "The respondent filed a suit for the grant of a permanent injunction restraining the appellant from interfering with the possession.\n",
    "The Court upheld the contentions of the appellant that the lease had terminated by efflux of time.\n",
    "The respondent 's 'suit was dismissed.\n",
    "An appeal was pre ferred.\n",
    "The appeal was dismissed on 21 August,.\n",
    "The appellant then instituted the suit in appeal claim ing possession from the respondent.\n",
    "The appellant contended that the respondent was a trespasser and claimed damages for unauthorised occupation.\n",
    "The respondent contended that he was still a tenant.\n",
    "The respondent claimed protection under the Mysore Tenants (Temporary Protection from Eviction) Act, 1961 being Act No. 15 of 1961.\n",
    "Section 3 of the Mysore Tenants (Temporary Protection from Evic .\n",
    "tion) Act, 1961 provided for prohibition against eviction.\n",
    "The appellant obtained a decree in the suit.\n",
    "The decree directed the respondent to deliver possession.\n",
    "The respond ent preferred an ' appeal.\n",
    "The High Court remanded the matter to the trial Court for assessment of damages.\n",
    "Upon remand the respondent applied for the amendment of the written statement.\n",
    "The respondent claimed protection under the Karnataka Land Reforms Act, 1961.\n",
    "It may be stated here that the Mysore Tenants (Temporary Protection from Eviction) Act, 1961 ceased to be in force in March, 1966.\n",
    "That is perhaps why the respondent made an applica tion for amendment of the written statement on 2 February 1973.\n",
    "The respondent contended relying on section 133 of the Karnataka Land Reforms Act, 1961 that the.\n",
    "suit should be stayed by the civil court and should be referred to the Tribunal for decision.\n",
    "Section 112(B)(b) of the Karnataka Land Reforms Act, 1961 confers power on the Tribunal to decide inter alia whether a person is a tenant or not.\n",
    "The respondent contended that he was a person who was deemed to be a tenant.\n",
    "The appellant opposed the application for stay of the suit by the civil court and referring to the Tribunal for decision under the Karnataka Land Reforms Act, 1961.\n",
    "The trial Court held that the land ' belonging to the appellant was exempted from the application of the provisions of the Land Reforms Act.\n",
    "The trial Court dismissed the application of the respondent.\n",
    "The respondent presented a revision petition t0 the High Court.\n",
    "The High Court reversed the decision of the trial Court and directed the trial Court to refer such of the issues which are required to be.\n",
    "decided by the Tribunal.\n",
    "271 Counsel for the respondent contended that the respondent is a tenant within the meaning of the word \"tenant\" defined in section 2(34) of the Karnataka Land Reforms Act, 1961.\n",
    "\"Tenant\" is defined to mean an agriculturist who cultivates personally the land he holds on lease from a landlord and includes (i) a person who is deemed to be a tenant under section 4 of the Karnataka Land Reforms Act, 1961, Section of the Karnataka Land Reforms Act, 1961 states that a person lawfully cultivating any land belonging to another person shall be deemed to be a tenant if such land is not cultivat ed personally by the owner and if such person is not (a) a member of the owner 's family, or (b) a servant or a hired labourer on wages, or (c) a mortgage in possession It was, therefore, said that the respondent could raise the con tention whether the respondent was a tenant or not.\n",
    "It was next contended that section 8 of the Karnataka Land Reforms Act, 1961 speaks of rent and rent is referable to tenant and therefore a dispute as to tenancy would be within the ambit of the Karnataka Land Reforms Act, 1961.\n",
    "Section 107 of the Karnataka Land Reforms Act, 1961 states that subject to the provisions of section 110 nothing in this Act, except section 8 shall apply to lands, inter alia (iii) belonging to or held on lease or from a local authority.\n",
    "There is no dispute that the land was given on lease by the local authority.\n",
    "There is also no 'dispute that the land belongs to the local authority.\n",
    "There is also no dispute that the lease was detrmined by efflux of time.\n",
    "The question whether the respondent is a tenant or deemed to be a tenant does not at all arise because the tenancy came to an end.\n",
    "The 'respondent thereafter was a trespasser.\n",
    "Section 107 of the Karnataka Land Reforms Act, 1961 makes it quite clear that the only provision which applies, inter alia, to lands belonging to or hold on lease or from a local authority is section 8.\n",
    "No other section of the Land Reforms Act applies to these lands.\n",
    "Section 8 of the Karna taka Land Reforms Act, 1961 deals with rent.\n",
    "The suit in the present case was not for recovery of rent.\n",
    "The suit is for recovery of possession and for damages, for unauthorised occupation of the respondent.\n",
    "Section 2 of the Karnataka Land Reforms Act, 1961 is not applicable.\n",
    "Therefore, no question can be referred for determination by the Tribunal under section 133.\n",
    "The Mysore Tenants (Temporary Protection from Eviction) Act, 1961 came into effect on 13 December, 1961.\n",
    "The Mysore Tenants (Temporary Protection from Eviction) Act, 1961 remained in force till the month of March, 1966.\n",
    "The re spondent could not draw any support from that Act for pro tection against eviction.\n",
    "The land in question was outside the applicability of the Mysore Tenants (Temporary Protec tion from Eviction) Act, 1961.\n",
    "Further the Act ceased to be in operation in 1966 and no question could be referred for determination as to whether the respondent was a tenant under the Mysore Tenants (Temporary Protection from Evic tion) Act, 1961 or not.\n",
    "The trial Court in the present case rightly said that it could not be said that there was any dispute as to tenancy.\n",
    "272 The respondent had filed a suit where he claimed to remain in possession.\n",
    "The suit of the respondent was dismissed.\n",
    "The appellant all along contended that the lease dated 14 September 1963 for a period of 5 years expired by efflux of time.\n",
    "The appellant claimed possession on the ground Of unauthorised occupation and claimed damages against the respondent, who was a trespasser.\n",
    "The High Court was clearly in error in referring to the Tribunal under the Karnataka Land Reforms Act 1961 determi nation of the plea taken by the respondent that he was pro tected by the Mysore Tenants (Temporary Protection from Eviction) Act 1961.\n",
    "Counsel for the respondent did not support the judgment on that ground.\n",
    "Counsel for the respondent contended that section 133 of the Karnataka Land Reforms Act 1961 excludes jurisdiction of Civil court in suits for possession where the defendant claims to be a tenant.\n",
    "The plea of the respondent is utterly unsound.\n",
    "Section 133 of the Karnataka Land Reforms Act 1961 cannot apply to lands which are held by a person on lease from the local authority or where the lease had ex pired and the local authority sues for possession on the ground that there is unauthorised occupation.\n",
    "No provision of the Karnataka Land Reforms Act can be relied upon to contend that there should be protection against recovery of possession by the local authority.\n",
    "For the foregoing reasons the judgment of the High Court is set aside.\n",
    "In view of the fact that no costs were al lowed by the High Court, there will be no order as to costs.\n",
    "M.R. Appeal allowed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('final_data.csv')\n",
    "tt = df['judgement'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jpsre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Training: 100%|██████████| 33477/33477 [19:46<00:00, 28.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3 - Loss: 0.2835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 33477/33477 [19:48<00:00, 28.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3 - Loss: 0.2523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 33477/33477 [20:26<00:00, 27.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/3 - Loss: 0.2421\n",
      "\n",
      "Generated Summary:\n",
      " The trial Court in the present case rightly said that it could not be said that there was any dispute as to tenancy.. The respondent filed a suit for the grant of a permanent injunction restraining the appellant from interfering with the possession.. 272 The respondent had filed a suit where he claimed to remain in possession.. In view of the fact that no costs were al lowed by the High Court, there will be no order as to costs.. The High Court remanded the matter to the trial Court for assessment of damages.. This appeal by special leave is from the judgment .dated 25 June, 1974 of the Karnataka High Court.. The appellant then instituted the suit in appeal claim ing possession from the respondent.. The question whether the respondent is a tenant or deemed to be a tenant does not at all arise because the tenancy came to an end.. The High Court reversed the decision of the trial Court and directed the trial Court to refer such of the issues which are required to be.. The respondent contended that he was a person who was deemed to be a tenant.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ROUGE: 100%|██████████| 204/204 [05:05<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROUGE Evaluation:\n",
      "ROUGE-1: 0.6010\n",
      "ROUGE-2: 0.4185\n",
      "ROUGE-L: 0.3266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def preprocess_data(texts, summaries):\n",
    "    processed_texts = []\n",
    "    processed_labels = []\n",
    "\n",
    "    for text, summary in zip(texts, summaries):\n",
    "        text_sentences = nltk.sent_tokenize(str(text))\n",
    "        summary_sentences = nltk.sent_tokenize(str(summary))\n",
    "\n",
    "        labels = [1 if sentence in summary_sentences else 0 for sentence in text_sentences]\n",
    "\n",
    "        processed_texts.extend(text_sentences)\n",
    "        processed_labels.extend(labels)\n",
    "\n",
    "    return processed_texts, processed_labels\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "class ExtractiveSummarizer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, n_heads=8):\n",
    "        super(ExtractiveSummarizer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim * 2, num_heads=n_heads)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=hidden_dim * 2, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded = self.dropout(self.embedding(input_ids)) \n",
    "        lstm_output, _ = self.lstm(embedded)\n",
    "        lstm_output = self.layer_norm(lstm_output)\n",
    "\n",
    "        lstm_output = lstm_output.permute(1, 0, 2)\n",
    "        attn_output, _ = self.attention(lstm_output, lstm_output, lstm_output)\n",
    "        attn_output = attn_output.permute(1, 0, 2)\n",
    "\n",
    "        conv_input = attn_output.permute(0, 2, 1)\n",
    "        conv_output = self.conv1d(conv_input)\n",
    "        conv_output = conv_output.permute(0, 2, 1)\n",
    "\n",
    "        pooled_output = torch.mean(conv_output, dim=1)\n",
    "\n",
    "        output = self.fc(pooled_output)\n",
    "\n",
    "        return torch.sigmoid(output).squeeze(1)\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def generate_summary(model, text, tokenizer, max_len, device, top_k=10):\n",
    "    model.eval()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    summaries = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask)\n",
    "            summaries.append((sentence, output.item()))\n",
    "\n",
    "    summaries.sort(key=lambda x: x[1], reverse=True)\n",
    "    return \". \".join([sentence for sentence, score in summaries[:top_k]])\n",
    "\n",
    "\n",
    "def evaluate_rouge(model, texts, references, tokenizer, max_len, device, top_k=10):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "\n",
    "    for text, reference in tqdm(zip(texts, references), total=len(texts), desc=\"Evaluating ROUGE\"):\n",
    "        generated_summary = generate_summary(model, text, tokenizer, max_len, device, top_k)\n",
    "        scores = scorer.score(reference, generated_summary)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "    avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "    avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "    print(\"\\nROUGE Evaluation:\")\n",
    "    print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n",
    "    print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n",
    "    print(f\"ROUGE-L: {avg_rougeL:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    VOCAB_SIZE = 30522\n",
    "    EMBEDDING_DIM = 256\n",
    "    HIDDEN_DIM = 128\n",
    "    OUTPUT_DIM = 1\n",
    "    N_LAYERS = 2\n",
    "    DROPOUT = 0.3\n",
    "    MAX_LEN = 1024\n",
    "    BATCH_SIZE = 4\n",
    "    LR = 1e-4\n",
    "    EPOCHS = 3\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "    df = pd.read_csv(\"final_data.csv\")\n",
    "    df = df.dropna(subset=[\"judgement\", \"summary\"])\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_texts, train_summaries = train_df[\"judgement\"].tolist(), train_df[\"summary\"].tolist()\n",
    "    val_texts, val_summaries = val_df[\"judgement\"].tolist(), val_df[\"summary\"].tolist()\n",
    "\n",
    "    train_texts, train_labels = preprocess_data(train_texts, train_summaries)\n",
    "    val_texts, val_labels = preprocess_data(val_texts, val_summaries)\n",
    "\n",
    "    train_dataset = TextDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "    val_dataset = TextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = ExtractiveSummarizer(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_model(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} - Loss: {train_loss:.4f}\")\n",
    "\n",
    "    example_text = df.iloc[0][\"judgement\"]\n",
    "    summary = generate_summary(model, example_text, tokenizer, MAX_LEN, DEVICE)\n",
    "    print(\"\\nGenerated Summary:\\n\", summary)\n",
    "\n",
    "    evaluate_rouge(model, val_df[\"judgement\"].tolist(), val_df[\"summary\"].tolist(), tokenizer, MAX_LEN, DEVICE, top_k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Summary:\n",
      "The complainant Savai Kala, the brother of the deceased saw the latter part of the occurrence when the deceased was being carried away by the accused. After hearing the learned counsel and examining the petition of appeal and after going through the relevant parts of the judgment of the High Court and the Sessions Court. The appeal is summarily dismissed under S 384 of the Code of Criminal Procedure.\n",
      "\n",
      "📚 IPC Sections Referenced:\n",
      "\n",
      "➡️ IPC_302\n",
      "   Offense   : Murder\n",
      "   Punishment: Death or Imprisonment for Life + Fine\n",
      "\n",
      "➡️ IPC_384\n",
      "   Offense   : Extortion\n",
      "   Punishment: 3 Years or Fine or Both\n",
      "\n",
      "➡️ IPC_379\n",
      "   Offense   : Theft\n",
      "   Punishment: 3 Years or Fine or Both\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from model import  ExtractiveSummarizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"saved_tokenizer/\")\n",
    "\n",
    "VOCAB_SIZE = 30522\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "MAX_LEN = 1024\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "\n",
    "class ExtractiveSummarizer(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM,\n",
    "                 hidden_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM,\n",
    "                 n_layers=N_LAYERS, dropout=DROPOUT, n_heads=8):\n",
    "        super(ExtractiveSummarizer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim * 2, num_heads=n_heads)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=hidden_dim * 2, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.dropout(self.embedding(input_ids))   \n",
    "        lstm_output, _ = self.lstm(embedded)   \n",
    "        lstm_output = self.layer_norm(lstm_output)\n",
    "\n",
    "        lstm_output = lstm_output.permute(1, 0, 2)  \n",
    "        attn_output, _ = self.attention(lstm_output, lstm_output, lstm_output)\n",
    "        attn_output = attn_output.permute(1, 0, 2)  \n",
    "\n",
    "        conv_input = attn_output.permute(0, 2, 1)   \n",
    "        conv_output = self.conv1d(conv_input)   \n",
    "        conv_output = conv_output.permute(0, 2, 1)  \n",
    "\n",
    "        pooled_output = torch.mean(conv_output, dim=1)   \n",
    "\n",
    "        output = self.fc(pooled_output)   \n",
    "\n",
    "        return torch.sigmoid(output).squeeze(1)   \n",
    "\n",
    "model = ExtractiveSummarizer()\n",
    "model.load_state_dict(torch.load(\"extractive_summarizer_model.pth\", map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")))\n",
    "model.eval()\n",
    "\n",
    "ipc_df = pd.read_csv(\"ipc_sections.csv\")\n",
    "ipc_lookup = {\n",
    "    row[\"Section\"].strip().upper(): {\"Offence\": row[\"Offense\"], \"Punishment\": row[\"Punishment\"]}\n",
    "    for _, row in ipc_df.iterrows()\n",
    "}\n",
    "\n",
    "def extract_ipc_sections(text):\n",
    "    matches = re.findall(r\"Section\\s+(\\d+[A-Z]?)\", text, flags=re.IGNORECASE)\n",
    "    return list(set([f\"IPC_{match.upper()}\" for match in matches]))\n",
    "\n",
    "def summarize_text(text, max_sentences=3):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    if len(sentences) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(input_ids)\n",
    "\n",
    "    if isinstance(scores, torch.Tensor):\n",
    "        scores = scores.squeeze().tolist()\n",
    "    if not isinstance(scores, list):\n",
    "        scores = [scores]\n",
    "\n",
    "    top_idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:max_sentences]\n",
    "    top_idxs.sort()   \n",
    "    summary = \" \".join([sentences[i] for i in top_idxs])\n",
    "    return summary\n",
    "\n",
    "def process_legal_text(text):\n",
    "    print(\"\\n📝 Summary:\")\n",
    "    summary = summarize_text(text)\n",
    "    print(summary)\n",
    "\n",
    "    print(\"\\n📚 IPC Sections Referenced:\\n\")\n",
    "    sections_found = extract_ipc_sections(text)\n",
    "\n",
    "    valid_sections = []\n",
    "    for sec in sections_found:\n",
    "        details = ipc_lookup.get(sec, None)\n",
    "        if details:\n",
    "            valid_sections.append((sec, details))\n",
    "\n",
    "    if not valid_sections:\n",
    "        print(\"⚠️ No valid IPC Sections referenced in the text.\")\n",
    "        return\n",
    "\n",
    "    for sec, details in valid_sections:\n",
    "        print(f\"➡️ {sec}\")\n",
    "        print(f\"   Offense   : {details['Offence']}\")\n",
    "        print(f\"   Punishment: {details['Punishment']}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_legal_text(tt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ptorch)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
